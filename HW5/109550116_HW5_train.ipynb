{"cells":[{"cell_type":"code","execution_count":669,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-20T19:06:27.521341Z","iopub.status.busy":"2022-12-20T19:06:27.520492Z","iopub.status.idle":"2022-12-20T19:06:38.195028Z","shell.execute_reply":"2022-12-20T19:06:38.193839Z","shell.execute_reply.started":"2022-12-20T19:06:27.521278Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/captcha-hacker/sample_submission.csv\n","/kaggle/input/captcha-hacker/test/task1/x4LPcV5n6IvLj4vz.png\n","/kaggle/input/captcha-hacker/test/task1/W88fVMlAs5IpsXn4.png\n","/kaggle/input/captcha-hacker/test/task1/ZWDL6pUMfPu5c9jh.png\n","...\n","/kaggle/input/captcha-hacker/test/task2/jMalnsI5a5IWxYAi.png\n","/kaggle/input/captcha-hacker/test/task2/ihE9HHgyOINGEMcO.png\n","/kaggle/input/captcha-hacker/test/task2/ZATEVW3P5s0akZjd.png\n","...\n","/kaggle/input/captcha-hacker/test/task3/cXBlxYfvQWbiK7dn.png\n","/kaggle/input/captcha-hacker/test/task3/5gEp1jR9jNNfuqlk.png\n","/kaggle/input/captcha-hacker/test/task3/hEQ0WQtB9B7j8C2f.png\n","...\n","/kaggle/input/captcha-hacker/train/annotations.csv\n","/kaggle/input/captcha-hacker/train/task1/H85RQ6dbWUvLSIDV.png\n","/kaggle/input/captcha-hacker/train/task1/n2GC8uY1N4QfvVxe.png\n","/kaggle/input/captcha-hacker/train/task1/XOqfRx2R6SnoEjFr.png\n","...\n","/kaggle/input/captcha-hacker/train/task2/Mr4B2zxXk92hyzn9.png\n","/kaggle/input/captcha-hacker/train/task2/SIuRCnlK8VS91FhX.png\n","/kaggle/input/captcha-hacker/train/task2/H323tAgDOLPa3ajU.png\n","...\n","/kaggle/input/captcha-hacker/train/task3/P0zteSLjXm3EcBRx.png\n","/kaggle/input/captcha-hacker/train/task3/lhqG7t9j2ZqoqcTD.png\n","/kaggle/input/captcha-hacker/train/task3/vGRIi80QVR1gpnjU.png\n","...\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames[:3]:\n","        print(os.path.join(dirname, filename))\n","    if len(filenames) > 3:\n","        print(\"...\")\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":670,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:38.199148Z","iopub.status.busy":"2022-12-20T19:06:38.198814Z","iopub.status.idle":"2022-12-20T19:06:38.209158Z","shell.execute_reply":"2022-12-20T19:06:38.207323Z","shell.execute_reply.started":"2022-12-20T19:06:38.199118Z"},"trusted":true},"outputs":[],"source":["import csv\n","import cv2\n","import numpy as np\n","import random\n","import tensorflow as tf\n","import os\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import io\n","from tqdm import tqdm\n","import torchvision.models as models\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":671,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:38.211316Z","iopub.status.busy":"2022-12-20T19:06:38.210958Z","iopub.status.idle":"2022-12-20T19:06:38.218226Z","shell.execute_reply":"2022-12-20T19:06:38.217026Z","shell.execute_reply.started":"2022-12-20T19:06:38.211278Z"},"trusted":true},"outputs":[],"source":["TRAIN_PATH = \"/kaggle/input/captcha-hacker/train\"\n","TEST_PATH = \"/kaggle/input/captcha-hacker/test\"\n","device = \"cuda\"\n","# try device = \"cuda\" \n","# and change your settings/accelerator to GPU if you want it to run faster"]},{"cell_type":"code","execution_count":673,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.209223Z","iopub.status.busy":"2022-12-20T19:06:39.208116Z","iopub.status.idle":"2022-12-20T19:06:39.215897Z","shell.execute_reply":"2022-12-20T19:06:39.214872Z","shell.execute_reply.started":"2022-12-20T19:06:39.209174Z"},"trusted":true},"outputs":[],"source":["transform_1= transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"]},{"cell_type":"code","execution_count":674,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.219588Z","iopub.status.busy":"2022-12-20T19:06:39.219281Z","iopub.status.idle":"2022-12-20T19:06:39.227705Z","shell.execute_reply":"2022-12-20T19:06:39.226680Z","shell.execute_reply.started":"2022-12-20T19:06:39.219548Z"},"trusted":true},"outputs":[],"source":["transform_2= transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),\n","    #transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"]},{"cell_type":"code","execution_count":675,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.229752Z","iopub.status.busy":"2022-12-20T19:06:39.229258Z","iopub.status.idle":"2022-12-20T19:06:39.241298Z","shell.execute_reply":"2022-12-20T19:06:39.240339Z","shell.execute_reply.started":"2022-12-20T19:06:39.229713Z"},"trusted":true},"outputs":[],"source":["transform_3 = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((384,288)),\n","    #384*288\n","    #transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"]},{"cell_type":"code","execution_count":676,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.243709Z","iopub.status.busy":"2022-12-20T19:06:39.242882Z","iopub.status.idle":"2022-12-20T19:06:39.253629Z","shell.execute_reply":"2022-12-20T19:06:39.252662Z","shell.execute_reply.started":"2022-12-20T19:06:39.243671Z"},"trusted":true},"outputs":[],"source":["class Task1Dataset(Dataset):\n","    def __init__(self, data, root, return_filename=False):\n","        self.data = [sample for sample in data if sample[0].startswith(\"task1\")]\n","        self.return_filename = return_filename\n","        self.root = root\n","    \n","    def __getitem__(self, index):\n","        filename, label = self.data[index]\n","        img = cv2.imread(f\"{self.root}/{filename}\")\n","#         img = cv2.resize(img, (32, 32))\n","#         img = np.mean(img, axis=2)\n","#         print(img.shape)\n","        img = transform_1(img)\n","        if self.return_filename:\n","            return torch.FloatTensor(img / 256), filename\n","        else:\n","            return torch.FloatTensor(img / 256), int(label)\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":677,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.257300Z","iopub.status.busy":"2022-12-20T19:06:39.256871Z","iopub.status.idle":"2022-12-20T19:06:39.267076Z","shell.execute_reply":"2022-12-20T19:06:39.266097Z","shell.execute_reply.started":"2022-12-20T19:06:39.257270Z"},"trusted":true},"outputs":[],"source":["class Task2Dataset(Dataset):\n","    def __init__(self, data, root, return_filename=False):\n","        self.data = [sample for sample in data if sample[0].startswith(\"task2\")]\n","        self.return_filename = return_filename\n","        self.root = root\n","    \n","    def __getitem__(self, index):\n","        filename, label = self.data[index]\n","        img = cv2.imread(f\"{self.root}/{filename}\")\n","        #img = io.read_image(f\"{self.root}/{filename}\")\n","        #img = cv2.resize(img, (224, 224))\n","        img = transform_2(img)\n","        if self.return_filename:\n","            return torch.FloatTensor((img ) / 255), filename\n","        else:\n","            return torch.FloatTensor((img ) / 255), label\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":678,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.271437Z","iopub.status.busy":"2022-12-20T19:06:39.271127Z","iopub.status.idle":"2022-12-20T19:06:39.279967Z","shell.execute_reply":"2022-12-20T19:06:39.278890Z","shell.execute_reply.started":"2022-12-20T19:06:39.271411Z"},"trusted":true},"outputs":[],"source":["class Task3Dataset(Dataset):\n","    def __init__(self, data, root, return_filename=False):\n","        self.data = [sample for sample in data if sample[0].startswith(\"task3\")]\n","        self.return_filename = return_filename\n","        self.root = root\n","    \n","    def __getitem__(self, index):\n","        filename, label = self.data[index]\n","        img = cv2.imread(f\"{self.root}/{filename}\")\n","        #img = io.read_image(f\"{self.root}/{filename}\")\n","        #img = cv2.resize(img, (224, 224))\n","        img = transform_3(img)\n","        if self.return_filename:\n","            return torch.FloatTensor((img ) / 255), filename\n","        else:\n","            return torch.FloatTensor((img ) / 255), label\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":679,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.282101Z","iopub.status.busy":"2022-12-20T19:06:39.281587Z","iopub.status.idle":"2022-12-20T19:06:39.292770Z","shell.execute_reply":"2022-12-20T19:06:39.291713Z","shell.execute_reply.started":"2022-12-20T19:06:39.281937Z"},"trusted":true},"outputs":[],"source":["class ResNet18_1(nn.Module):\n","    def __init__(self):\n","        super(ResNet18_1, self).__init__()\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.fc = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        logits = self.model(x)\n","        return logits"]},{"cell_type":"code","execution_count":680,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.296014Z","iopub.status.busy":"2022-12-20T19:06:39.295629Z","iopub.status.idle":"2022-12-20T19:06:39.303730Z","shell.execute_reply":"2022-12-20T19:06:39.302798Z","shell.execute_reply.started":"2022-12-20T19:06:39.295924Z"},"trusted":true},"outputs":[],"source":["class ResNet18_2(nn.Module):\n","    def __init__(self):\n","        super(ResNet18_2, self).__init__()\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.fc = nn.Linear(512, 72)\n","\n","    def forward(self, x):\n","        logits = self.model(x)\n","        return logits"]},{"cell_type":"code","execution_count":681,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.305711Z","iopub.status.busy":"2022-12-20T19:06:39.305269Z","iopub.status.idle":"2022-12-20T19:06:39.314525Z","shell.execute_reply":"2022-12-20T19:06:39.313586Z","shell.execute_reply.started":"2022-12-20T19:06:39.305676Z"},"trusted":true},"outputs":[],"source":["class ResNet18_3(nn.Module):\n","    def __init__(self):\n","        super(ResNet18_3, self).__init__()\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.fc = nn.Linear(512, 144)\n","\n","    def forward(self, x):\n","        logits = self.model(x)\n","        return logits"]},{"cell_type":"code","execution_count":682,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.317943Z","iopub.status.busy":"2022-12-20T19:06:39.317674Z","iopub.status.idle":"2022-12-20T19:06:39.345817Z","shell.execute_reply":"2022-12-20T19:06:39.344751Z","shell.execute_reply.started":"2022-12-20T19:06:39.317918Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["train_data = []\n","val_data = []\n","\n","file = open(f'{TRAIN_PATH}/annotations.csv', newline='')\n","with file as csvfile:\n","    for row in csv.reader(csvfile, delimiter=','):\n","        if random.random() < 0.7:\n","            train_data.append(row)\n","        else:\n","            val_data.append(row)\n","            \n","train_ds = Task1Dataset(train_data, root=TRAIN_PATH)\n","train_dl = DataLoader(train_ds, batch_size=64, num_workers=4, drop_last=True, shuffle=True)\n","\n","val_ds = Task1Dataset(val_data, root=TRAIN_PATH)\n","val_dl = DataLoader(val_ds, batch_size=64, num_workers=4, drop_last=False, shuffle=False)"]},{"cell_type":"code","execution_count":683,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:06:39.347720Z","iopub.status.busy":"2022-12-20T19:06:39.347365Z","iopub.status.idle":"2022-12-20T19:07:38.912289Z","shell.execute_reply":"2022-12-20T19:07:38.910092Z","shell.execute_reply.started":"2022-12-20T19:06:39.347684Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [0]\n","accuracy (validation): tensor(0.0961, device='cuda:0')\n","Epoch [1]\n","accuracy (validation): tensor(0.0720, device='cuda:0')\n","Epoch [2]\n","accuracy (validation): tensor(0.2624, device='cuda:0')\n","Epoch [3]\n","accuracy (validation): tensor(0.9897, device='cuda:0')\n","Epoch [4]\n","accuracy (validation): tensor(0.9966, device='cuda:0')\n","Epoch [5]\n","accuracy (validation): tensor(0.9949, device='cuda:0')\n","Epoch [6]\n","accuracy (validation): tensor(0.9983, device='cuda:0')\n","Epoch [7]\n","accuracy (validation): tensor(1., device='cuda:0')\n","Epoch [8]\n","accuracy (validation): tensor(1., device='cuda:0')\n","Epoch [9]\n","accuracy (validation): tensor(1., device='cuda:0')\n"]}],"source":["model = ResNet18_1().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","loss_fn = nn.CrossEntropyLoss()\n","PATH_1 = \"model_1.pt\"\n","\n","for epoch in range(10):\n","    print(f\"Epoch [{epoch}]\")\n","    model.train()\n","    for image, label in train_dl:\n","        image = image.to(device)\n","        label = label.to(device)\n","        #print(image.shape)\n","        \n","        pred = model(image)\n","        loss = loss_fn(pred, label)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    sample_count = 0\n","    correct_count = 0\n","    model.eval()\n","    for image, label in val_dl:\n","        image = image.to(device)\n","        label = label.to(device)\n","        #image = image.permute(0,3,1,2)\n","        pred = model(image)\n","        loss = loss_fn(pred, label)\n","        \n","        pred = torch.argmax(pred, dim=1)\n","        \n","        sample_count += len(image)\n","        correct_count += (label == pred).sum()\n","        \n","    print(\"accuracy (validation):\", correct_count / sample_count)\n","torch.save(model.state_dict(), PATH_1)"]},{"cell_type":"code","execution_count":685,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:07:55.109189Z","iopub.status.busy":"2022-12-20T19:07:55.108720Z","iopub.status.idle":"2022-12-20T19:09:42.490784Z","shell.execute_reply":"2022-12-20T19:09:42.489583Z","shell.execute_reply.started":"2022-12-20T19:07:55.109138Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [0]\n","accuracy (validation): 0.0\n","Epoch [1]\n","accuracy (validation): 0.9577278731836195\n","Epoch [2]\n","accuracy (validation): 0.9947159841479525\n","Epoch [3]\n","accuracy (validation): 0.9933949801849405\n","Epoch [4]\n","accuracy (validation): 0.9920739762219286\n","Epoch [5]\n","accuracy (validation): 0.9749009247027741\n","Epoch [6]\n","accuracy (validation): 0.9458388375165125\n","Epoch [7]\n","accuracy (validation): 0.9788639365918098\n","Epoch [8]\n","accuracy (validation): 0.9630118890356671\n","Epoch [9]\n","accuracy (validation): 0.9933949801849405\n","Epoch [10]\n","accuracy (validation): 0.9973579920739762\n","Epoch [11]\n","accuracy (validation): 1.0\n","Epoch [12]\n","accuracy (validation): 1.0\n","Epoch [13]\n","accuracy (validation): 0.9920739762219286\n","Epoch [14]\n","accuracy (validation): 1.0\n"]}],"source":["train_ds = Task2Dataset(train_data, root=TRAIN_PATH)\n","train_dl = DataLoader(train_ds, batch_size=32, num_workers=2, drop_last=True, shuffle=True)\n","\n","val_ds = Task2Dataset(val_data, root=TRAIN_PATH)\n","val_dl = DataLoader(val_ds, batch_size=32, num_workers=2, drop_last=False, shuffle=False)\n","\n","model = ResNet18_2().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","loss_fn = nn.MultiLabelSoftMarginLoss()\n","\n","PATH_2 = \"model_2.pt\"\n","for epoch in range(15):\n","    print(f\"Epoch [{epoch}]\")\n","    model.train()\n","    for image, label in train_dl:\n","        tt = list()\n","        #print (label)\n","        for i in label :\n","            temp = np.zeros(72)\n","            count=0\n","            for j in i:\n","                if count==0:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87]=1\n","                    else:\n","                        temp[int(j)]=1\n","                else:\n","                    \n","                    if(ord(j)>90):\n","                        temp[ord(j)-87+36]=1\n","                    else:\n","                        temp[int(j)+36]=1\n","                count+=1\n","            tt.append(temp)\n","        tt = np.array(tt)\n","        tt=torch.from_numpy(tt)\n","        tt=tt.to(device)\n","        image = image.to(device)\n","        #print(image.shape)\n","        #image = image.permute(0,3,1,2)\n","        pred = model(image)\n","        loss = loss_fn(pred, tt)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    sample_count = 0\n","    correct_count = 0\n","    model.eval()\n","    for image, label in val_dl:\n","\n","        tt = list()\n","        #print (label)\n","        for i in label :\n","            temp = np.zeros(72)\n","            count=0\n","            for j in i:\n","                if count==0:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87]=1\n","                    else:\n","                        temp[int(j)]=1\n","                else:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87+36]=1\n","                    else:\n","                        temp[int(j)+36]=1\n","                count+=1\n","            tt.append(temp)\n","        tt = np.array(tt)\n","        ttt=torch.from_numpy(tt)\n","        ttt=ttt.to(device)\n","        image = image.to(device)\n","        #image = image.permute(0,3,1,2)\n","        pred = model(image)\n","        #print(pred)\n","        loss = loss_fn(pred, ttt)\n","        \n","        pred = pred.data.cpu().numpy()\n","        tt1 = list()\n","        for j in range(pred.shape[0]):\n","            temp = np.zeros(72)\n","            max=-10000\n","            index=0\n","            for i in range(36):\n","                if pred[j][i] > max:\n","                    index=i\n","                    max=pred[j][i]\n","            temp[index]=1\n","            \n","            max=-10000\n","            index=0\n","            for i in range(36):\n","                if pred[j][i+36] > max:\n","                    index=i+36\n","                    max=pred[j][i+36]\n","            temp[index]=1\n","            tt1.append(temp)\n","        tt1 = np.array(tt1)\n","        \n","        #print (tt1[0])\n","        #print (pred[0][0])\n","        #print (pred)\n","        sample_count += len(image)\n","        for i in range(tt.shape[0]):\n","            #print (tt[i]-tt1[i])\n","            #print (\"\\n\")\n","            if ((tt[i]==tt1[i]).all()):\n","                correct_count += 1\n","        \n","    \n","    #print (sample_count)\n","    print(\"accuracy (validation):\", correct_count / sample_count)\n","torch.save(model.state_dict(), PATH_2)"]},{"cell_type":"code","execution_count":687,"metadata":{"execution":{"iopub.execute_input":"2022-12-20T19:09:49.443041Z","iopub.status.busy":"2022-12-20T19:09:49.442573Z","iopub.status.idle":"2022-12-20T19:17:17.613517Z","shell.execute_reply":"2022-12-20T19:17:17.612034Z","shell.execute_reply.started":"2022-12-20T19:09:49.442998Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [0]\n","918\n","accuracy (validation): 0.0\n","Epoch [1]\n","918\n","accuracy (validation): 0.0\n","Epoch [2]\n","918\n","accuracy (validation): 0.0\n","Epoch [3]\n","918\n","accuracy (validation): 0.08823529411764706\n","Epoch [4]\n","918\n","accuracy (validation): 0.3137254901960784\n","Epoch [5]\n","918\n","accuracy (validation): 0.6677559912854031\n","Epoch [6]\n","918\n","accuracy (validation): 0.8213507625272332\n","Epoch [7]\n","918\n","accuracy (validation): 0.9041394335511983\n","Epoch [8]\n","918\n","accuracy (validation): 0.9193899782135077\n","Epoch [9]\n","918\n","accuracy (validation): 0.9215686274509803\n","Epoch [10]\n","918\n","accuracy (validation): 0.9411764705882353\n","Epoch [11]\n","918\n","accuracy (validation): 0.9400871459694989\n","Epoch [12]\n","918\n","accuracy (validation): 0.9575163398692811\n","Epoch [13]\n","918\n","accuracy (validation): 0.9564270152505446\n","Epoch [14]\n","918\n","accuracy (validation): 0.9629629629629629\n","Epoch [15]\n","918\n","accuracy (validation): 0.9684095860566448\n","Epoch [16]\n","918\n","accuracy (validation): 0.9694989106753813\n","Epoch [17]\n","918\n","accuracy (validation): 0.9651416122004357\n","Epoch [18]\n","918\n","accuracy (validation): 0.9705882352941176\n","Epoch [19]\n","918\n","accuracy (validation): 0.9727668845315904\n","Epoch [20]\n","918\n","accuracy (validation): 0.9694989106753813\n","Epoch [21]\n","918\n","accuracy (validation): 0.9705882352941176\n","Epoch [22]\n","918\n","accuracy (validation): 0.9705882352941176\n","Epoch [23]\n","918\n","accuracy (validation): 0.9586056644880174\n","Epoch [24]\n","918\n","accuracy (validation): 0.9498910675381264\n","Epoch [25]\n","918\n","accuracy (validation): 0.644880174291939\n","Epoch [26]\n","918\n","accuracy (validation): 0.8474945533769063\n","Epoch [27]\n","918\n","accuracy (validation): 0.9466230936819172\n","Epoch [28]\n","918\n","accuracy (validation): 0.9662309368191722\n","Epoch [29]\n","918\n","accuracy (validation): 0.9662309368191722\n"]}],"source":["train_ds = Task3Dataset(train_data, root=TRAIN_PATH)\n","train_dl = DataLoader(train_ds, batch_size=32, num_workers=2, drop_last=True, shuffle=True)\n","\n","val_ds = Task3Dataset(val_data, root=TRAIN_PATH)\n","val_dl = DataLoader(val_ds, batch_size=32, num_workers=2, drop_last=False, shuffle=False)\n","\n","model = ResNet18_3().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","loss_fn = nn.MultiLabelSoftMarginLoss()\n","\n","PATH_3 = \"model_3.pt\"\n","for epoch in range(30):\n","    print(f\"Epoch [{epoch}]\")\n","    model.train()\n","    for image, label in train_dl:\n","        tt = list()\n","        #print (label)\n","        for i in label :\n","            temp = np.zeros(144)\n","            count=0\n","            for j in i:\n","                if count==0:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87]=1\n","                    else:\n","                        temp[int(j)]=1\n","                elif count==1:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87+36]=1\n","                    else:\n","                        temp[int(j)+36]=1\n","                elif count==2:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87+72]=1\n","                    else:\n","                        temp[int(j)+72]=1\n","                elif count==3:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87+108]=1\n","                    else:\n","                        temp[int(j)+108]=1\n","                count+=1\n","            tt.append(temp)\n","        tt = np.array(tt)\n","        tt=torch.from_numpy(tt)\n","        tt=tt.to(device)\n","        image = image.to(device)\n","        #image = image.permute(0,3,1,2)\n","        #print(image.shape)\n","        pred = model(image)\n","        loss = loss_fn(pred, tt)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    sample_count = 0\n","    correct_count = 0\n","    model.eval()\n","    for image, label in val_dl:\n","        tt = list()\n","        for i in label :\n","            temp = np.zeros(144)\n","            count=0\n","            for j in i:\n","                if count==0:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87]=1\n","                    else:\n","                        temp[int(j)]=1\n","                elif count==1:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87+36]=1\n","                    else:\n","                        temp[int(j)+36]=1\n","                elif count==2:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87+72]=1\n","                    else:\n","                        temp[int(j)+72]=1\n","                elif count==3:\n","                    if(ord(j)>90):\n","                        temp[ord(j)-87+108]=1\n","                    else:\n","                        temp[int(j)+108]=1\n","                count+=1\n","            tt.append(temp)\n","        tt = np.array(tt)\n","        ttt=torch.from_numpy(tt)\n","        ttt=ttt.to(device)\n","        image = image.to(device)\n","        #image = image.permute(0,3,1,2)\n","        #print(image.shape)\n","        pred = model(image)\n","        #print(pred)\n","        loss = loss_fn(pred, ttt)\n","        pred = pred.data.cpu().numpy()\n","        tt1 = list()\n","        for j in range(pred.shape[0]):\n","            temp = np.zeros(144)\n","            max=-10000\n","            index=0\n","            for i in range(36):\n","                if pred[j][i] > max:\n","                    index=i\n","                    max=pred[j][i]\n","            temp[index]=1\n","            max=-10000\n","            index=0\n","            for i in range(36):\n","                if pred[j][i+36] > max:\n","                    index=i+36\n","                    max=pred[j][i+36]\n","            temp[index]=1\n","            max=-10000\n","            index=0\n","            for i in range(36):\n","                if pred[j][i+72] > max:\n","                    index=i+72\n","                    max=pred[j][i+72]\n","            temp[index]=1\n","            max=-10000\n","            index=0\n","            for i in range(36):\n","                if pred[j][i+108] > max:\n","                    index=i+108\n","                    max=pred[j][i+108]\n","            temp[index]=1\n","            tt1.append(temp)\n","            #print(temp)\n","        tt1 = np.array(tt1)\n","        #print(tt1)\n","        #print (tt1[0])\n","        #print (pred[0][0])\n","        #print (pred)\n","        sample_count += len(image)\n","        for i in range(tt.shape[0]):\n","            #print (tt[i]-tt1[i])\n","            #print (\"\\n\")\n","            if ((tt[i]==tt1[i]).all()):\n","                correct_count += 1\n","        \n","    print (sample_count)\n","    print(\"accuracy (validation):\", correct_count / sample_count)\n","torch.save(model.state_dict(), PATH_3)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"vscode":{"interpreter":{"hash":"4a3dec92162c0bde73bd3e7e02d72e965c32f345ff10be2f6a2ec3010e485f60"}}},"nbformat":4,"nbformat_minor":4}
